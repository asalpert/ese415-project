{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f96f268-eb20-4726-85b9-f3d02f976681",
   "metadata": {},
   "source": [
    "# ESE 415 Final Project - Analyzing Data Scientist Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a3463-df16-4743-8371-1d31e5ca38ba",
   "metadata": {},
   "source": [
    "## By: Abigail Alpert and Kevin Yan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f16705-6c1f-49f2-a1cb-967e2903f7c9",
   "metadata": {},
   "source": [
    "# Data exploration and preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04919967-5d6c-45cb-ac3e-eb99404e4121",
   "metadata": {},
   "source": [
    "### Dataset Columns\n",
    "1. **work_year** : The year the salary was paid.\n",
    "\n",
    "2. **experience_level** : The experience level in the job during the year\n",
    "\n",
    "3. **employment_type** : The type of employment for the role\n",
    "\n",
    "4. **job_title** : The role worked in during the year.\n",
    "\n",
    "5. **salary** : The total gross salary amount paid.\n",
    "\n",
    "6. **salary_currency** : The currency of the salary paid as an ISO 4217 currency code.\n",
    "\n",
    "7. **salaryinusd** : The salary in USD\n",
    "\n",
    "8. **employee_residence** : Employee's primary country of residence in during the work year as an ISO 3166 country code.\n",
    "\n",
    "9. **remote_ratio** : The overall amount of work done remotely\n",
    "\n",
    "10. **company_location** : The country of the employer's main office or contracting branch\n",
    "\n",
    "11. **company_size** : The median number of people that worked for the company during the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca987961-a3ca-488b-865d-34eec2ca4df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0541b066-b106-43d9-9ddc-e4beff7579b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>70000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>79833</td>\n",
       "      <td>DE</td>\n",
       "      <td>0</td>\n",
       "      <td>DE</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Scientist</td>\n",
       "      <td>260000</td>\n",
       "      <td>USD</td>\n",
       "      <td>260000</td>\n",
       "      <td>JP</td>\n",
       "      <td>0</td>\n",
       "      <td>JP</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>85000</td>\n",
       "      <td>GBP</td>\n",
       "      <td>109024</td>\n",
       "      <td>GB</td>\n",
       "      <td>50</td>\n",
       "      <td>GB</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Product Data Analyst</td>\n",
       "      <td>20000</td>\n",
       "      <td>USD</td>\n",
       "      <td>20000</td>\n",
       "      <td>HN</td>\n",
       "      <td>0</td>\n",
       "      <td>HN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>150000</td>\n",
       "      <td>USD</td>\n",
       "      <td>150000</td>\n",
       "      <td>US</td>\n",
       "      <td>50</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>154000</td>\n",
       "      <td>USD</td>\n",
       "      <td>154000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>126000</td>\n",
       "      <td>USD</td>\n",
       "      <td>126000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>129000</td>\n",
       "      <td>USD</td>\n",
       "      <td>129000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>2022</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>150000</td>\n",
       "      <td>USD</td>\n",
       "      <td>150000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2022</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>AI Scientist</td>\n",
       "      <td>200000</td>\n",
       "      <td>USD</td>\n",
       "      <td>200000</td>\n",
       "      <td>IN</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     work_year experience_level employment_type                   job_title  \\\n",
       "0         2020               MI              FT              Data Scientist   \n",
       "1         2020               SE              FT  Machine Learning Scientist   \n",
       "2         2020               SE              FT           Big Data Engineer   \n",
       "3         2020               MI              FT        Product Data Analyst   \n",
       "4         2020               SE              FT   Machine Learning Engineer   \n",
       "..         ...              ...             ...                         ...   \n",
       "602       2022               SE              FT               Data Engineer   \n",
       "603       2022               SE              FT               Data Engineer   \n",
       "604       2022               SE              FT                Data Analyst   \n",
       "605       2022               SE              FT                Data Analyst   \n",
       "606       2022               MI              FT                AI Scientist   \n",
       "\n",
       "     salary salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
       "0     70000             EUR          79833                 DE             0   \n",
       "1    260000             USD         260000                 JP             0   \n",
       "2     85000             GBP         109024                 GB            50   \n",
       "3     20000             USD          20000                 HN             0   \n",
       "4    150000             USD         150000                 US            50   \n",
       "..      ...             ...            ...                ...           ...   \n",
       "602  154000             USD         154000                 US           100   \n",
       "603  126000             USD         126000                 US           100   \n",
       "604  129000             USD         129000                 US             0   \n",
       "605  150000             USD         150000                 US           100   \n",
       "606  200000             USD         200000                 IN           100   \n",
       "\n",
       "    company_location company_size  \n",
       "0                 DE            L  \n",
       "1                 JP            S  \n",
       "2                 GB            M  \n",
       "3                 HN            S  \n",
       "4                 US            L  \n",
       "..               ...          ...  \n",
       "602               US            M  \n",
       "603               US            M  \n",
       "604               US            M  \n",
       "605               US            M  \n",
       "606               US            L  \n",
       "\n",
       "[607 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ds_salaries.csv', index_col = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3aec3-3477-4957-80d1-b75236fb0118",
   "metadata": {},
   "source": [
    "First off, basic data cleaning (dropping duplicates and NA's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37732a89-4d8b-4991-88a0-4c9d6ed16fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.dropna(subset=['salary_in_usd']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a78fd2-be42-47f4-9a33-e250ad4c36c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's create a couple flags/binary variables that could be useful for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1674a151-994c-4a89-aee3-ebd38db79f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['is_manager'] = df['job_title'].str.contains('Manager|Lead|Director|Head', case=False).astype(int)\n",
    "df['is_remote'] = (df['remote_ratio'] == 100).astype(int)\n",
    "df['is_hybrid'] = ((df['remote_ratio'] > 0) & (df['remote_ratio'] < 100)).astype(int)\n",
    "df['same_country'] = (df['employee_residence'] == df['company_location']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6011c08f-7ca0-47e8-84a7-24bb9b84f603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Job Titles: 50\n",
      "Unique Employee Residences: 57\n",
      "Unique Company Locations: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Job Titles:\",len(df['job_title'].unique()))\n",
    "print(\"Unique Employee Residences:\",len(df['employee_residence'].unique()))\n",
    "print(\"Unique Company Locations:\",len(df['company_location'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b397d0-d112-499b-aed2-6f0105c52b2d",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> Check if standardizing makes sense here. My concern is that it'll make our predictions hard to interpret </span>\n",
    "\n",
    "Let's standardize the numerical features to help the model converge faster. The different numerical features have very different magnitudes which could cause issues during gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24c8183-8114-401a-a13a-ceb4f048220e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "      <th>is_manager</th>\n",
       "      <th>is_remote</th>\n",
       "      <th>is_hybrid</th>\n",
       "      <th>same_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.956361</td>\n",
       "      <td>-0.167734</td>\n",
       "      <td>-0.426180</td>\n",
       "      <td>-1.710815</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>EUR</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.956361</td>\n",
       "      <td>-0.048869</td>\n",
       "      <td>2.068630</td>\n",
       "      <td>-1.710815</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>JP</td>\n",
       "      <td>JP</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.956361</td>\n",
       "      <td>-0.158350</td>\n",
       "      <td>-0.021966</td>\n",
       "      <td>-0.487257</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>GBP</td>\n",
       "      <td>GB</td>\n",
       "      <td>GB</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.956361</td>\n",
       "      <td>-0.199014</td>\n",
       "      <td>-1.254701</td>\n",
       "      <td>-1.710815</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Product Data Analyst</td>\n",
       "      <td>USD</td>\n",
       "      <td>HN</td>\n",
       "      <td>HN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.956361</td>\n",
       "      <td>-0.117686</td>\n",
       "      <td>0.545437</td>\n",
       "      <td>-0.487257</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0.910939</td>\n",
       "      <td>-0.115183</td>\n",
       "      <td>0.600826</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.910939</td>\n",
       "      <td>-0.132700</td>\n",
       "      <td>0.213104</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0.910939</td>\n",
       "      <td>-0.130823</td>\n",
       "      <td>0.254645</td>\n",
       "      <td>-1.710815</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.910939</td>\n",
       "      <td>-0.117686</td>\n",
       "      <td>0.545437</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.910939</td>\n",
       "      <td>-0.086406</td>\n",
       "      <td>1.237797</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>AI Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>IN</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>565 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    work_year    salary salary_in_usd remote_ratio experience_level  \\\n",
       "0   -1.956361 -0.167734     -0.426180    -1.710815               MI   \n",
       "1   -1.956361 -0.048869      2.068630    -1.710815               SE   \n",
       "2   -1.956361 -0.158350     -0.021966    -0.487257               SE   \n",
       "3   -1.956361 -0.199014     -1.254701    -1.710815               MI   \n",
       "4   -1.956361 -0.117686      0.545437    -0.487257               SE   \n",
       "..        ...       ...           ...          ...              ...   \n",
       "560  0.910939 -0.115183      0.600826     0.736300               SE   \n",
       "561  0.910939 -0.132700      0.213104     0.736300               SE   \n",
       "562  0.910939 -0.130823      0.254645    -1.710815               SE   \n",
       "563  0.910939 -0.117686      0.545437     0.736300               SE   \n",
       "564  0.910939 -0.086406      1.237797     0.736300               MI   \n",
       "\n",
       "    employment_type                   job_title salary_currency  \\\n",
       "0                FT              Data Scientist             EUR   \n",
       "1                FT  Machine Learning Scientist             USD   \n",
       "2                FT           Big Data Engineer             GBP   \n",
       "3                FT        Product Data Analyst             USD   \n",
       "4                FT   Machine Learning Engineer             USD   \n",
       "..              ...                         ...             ...   \n",
       "560              FT               Data Engineer             USD   \n",
       "561              FT               Data Engineer             USD   \n",
       "562              FT                Data Analyst             USD   \n",
       "563              FT                Data Analyst             USD   \n",
       "564              FT                AI Scientist             USD   \n",
       "\n",
       "    employee_residence company_location company_size is_manager is_remote  \\\n",
       "0                   DE               DE            L          0         0   \n",
       "1                   JP               JP            S          0         0   \n",
       "2                   GB               GB            M          0         0   \n",
       "3                   HN               HN            S          0         0   \n",
       "4                   US               US            L          0         0   \n",
       "..                 ...              ...          ...        ...       ...   \n",
       "560                 US               US            M          0         1   \n",
       "561                 US               US            M          0         1   \n",
       "562                 US               US            M          0         0   \n",
       "563                 US               US            M          0         1   \n",
       "564                 IN               US            L          0         1   \n",
       "\n",
       "    is_hybrid same_country  \n",
       "0           0            1  \n",
       "1           0            1  \n",
       "2           1            1  \n",
       "3           0            1  \n",
       "4           1            1  \n",
       "..        ...          ...  \n",
       "560         0            1  \n",
       "561         0            1  \n",
       "562         0            1  \n",
       "563         0            1  \n",
       "564         0            0  \n",
       "\n",
       "[565 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = ['work_year', 'salary', 'salary_in_usd', 'remote_ratio']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_standardized = preprocessor.fit_transform(df)\n",
    "\n",
    "all_feature_names = list(num_cols) + [\n",
    "    col for col in df.columns \n",
    "    if col not in num_cols\n",
    "]\n",
    "\n",
    "standardized_df = pd.DataFrame(X_standardized, columns=all_feature_names)\n",
    "\n",
    "standardized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6b048-8056-4f62-b37d-f89b16eea03f",
   "metadata": {},
   "source": [
    "Unfortunately, it seems like these categorical variables have many possibilities, so performing one-hot encoding on these columns may create too many columns to be feasible. We will have to find other ways to deal with these columns, as we believe that the job title will play a strong role in predicting salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7b239-938d-4226-8c88-a28a547c1c14",
   "metadata": {
    "tags": []
   },
   "source": [
    "With that being said, let's handle the lower-cardinal categorical features with one-hot encoding. Note that we ignore salary_currency as we do not think it will be predictive given we have the salary_in_usd column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5fd4d83-3d8c-4979-a5db-0e9ea73e80db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_level_EN</th>\n",
       "      <th>experience_level_EX</th>\n",
       "      <th>experience_level_MI</th>\n",
       "      <th>experience_level_SE</th>\n",
       "      <th>employment_type_CT</th>\n",
       "      <th>employment_type_FL</th>\n",
       "      <th>employment_type_FT</th>\n",
       "      <th>employment_type_PT</th>\n",
       "      <th>company_size_L</th>\n",
       "      <th>company_size_M</th>\n",
       "      <th>...</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>company_location</th>\n",
       "      <th>is_manager</th>\n",
       "      <th>is_remote</th>\n",
       "      <th>is_hybrid</th>\n",
       "      <th>same_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426180</td>\n",
       "      <td>-1.710815</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>EUR</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.068630</td>\n",
       "      <td>-1.710815</td>\n",
       "      <td>Machine Learning Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>JP</td>\n",
       "      <td>JP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021966</td>\n",
       "      <td>-0.487257</td>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>GBP</td>\n",
       "      <td>GB</td>\n",
       "      <td>GB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.254701</td>\n",
       "      <td>-1.710815</td>\n",
       "      <td>Product Data Analyst</td>\n",
       "      <td>USD</td>\n",
       "      <td>HN</td>\n",
       "      <td>HN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545437</td>\n",
       "      <td>-0.487257</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600826</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213104</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254645</td>\n",
       "      <td>-1.710815</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545437</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.237797</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>AI Scientist</td>\n",
       "      <td>USD</td>\n",
       "      <td>IN</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>565 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    experience_level_EN experience_level_EX experience_level_MI  \\\n",
       "0                   0.0                 0.0                 1.0   \n",
       "1                   0.0                 0.0                 0.0   \n",
       "2                   0.0                 0.0                 0.0   \n",
       "3                   0.0                 0.0                 1.0   \n",
       "4                   0.0                 0.0                 0.0   \n",
       "..                  ...                 ...                 ...   \n",
       "560                 0.0                 0.0                 0.0   \n",
       "561                 0.0                 0.0                 0.0   \n",
       "562                 0.0                 0.0                 0.0   \n",
       "563                 0.0                 0.0                 0.0   \n",
       "564                 0.0                 0.0                 1.0   \n",
       "\n",
       "    experience_level_SE employment_type_CT employment_type_FL  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   1.0                0.0                0.0   \n",
       "2                   1.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   1.0                0.0                0.0   \n",
       "..                  ...                ...                ...   \n",
       "560                 1.0                0.0                0.0   \n",
       "561                 1.0                0.0                0.0   \n",
       "562                 1.0                0.0                0.0   \n",
       "563                 1.0                0.0                0.0   \n",
       "564                 0.0                0.0                0.0   \n",
       "\n",
       "    employment_type_FT employment_type_PT company_size_L company_size_M  ...  \\\n",
       "0                  1.0                0.0            1.0            0.0  ...   \n",
       "1                  1.0                0.0            0.0            0.0  ...   \n",
       "2                  1.0                0.0            0.0            1.0  ...   \n",
       "3                  1.0                0.0            0.0            0.0  ...   \n",
       "4                  1.0                0.0            1.0            0.0  ...   \n",
       "..                 ...                ...            ...            ...  ...   \n",
       "560                1.0                0.0            0.0            1.0  ...   \n",
       "561                1.0                0.0            0.0            1.0  ...   \n",
       "562                1.0                0.0            0.0            1.0  ...   \n",
       "563                1.0                0.0            0.0            1.0  ...   \n",
       "564                1.0                0.0            1.0            0.0  ...   \n",
       "\n",
       "    salary_in_usd remote_ratio                   job_title salary_currency  \\\n",
       "0       -0.426180    -1.710815              Data Scientist             EUR   \n",
       "1        2.068630    -1.710815  Machine Learning Scientist             USD   \n",
       "2       -0.021966    -0.487257           Big Data Engineer             GBP   \n",
       "3       -1.254701    -1.710815        Product Data Analyst             USD   \n",
       "4        0.545437    -0.487257   Machine Learning Engineer             USD   \n",
       "..            ...          ...                         ...             ...   \n",
       "560      0.600826     0.736300               Data Engineer             USD   \n",
       "561      0.213104     0.736300               Data Engineer             USD   \n",
       "562      0.254645    -1.710815                Data Analyst             USD   \n",
       "563      0.545437     0.736300                Data Analyst             USD   \n",
       "564      1.237797     0.736300                AI Scientist             USD   \n",
       "\n",
       "    employee_residence company_location is_manager is_remote is_hybrid  \\\n",
       "0                   DE               DE          0         0         0   \n",
       "1                   JP               JP          0         0         0   \n",
       "2                   GB               GB          0         0         1   \n",
       "3                   HN               HN          0         0         0   \n",
       "4                   US               US          0         0         1   \n",
       "..                 ...              ...        ...       ...       ...   \n",
       "560                 US               US          0         1         0   \n",
       "561                 US               US          0         1         0   \n",
       "562                 US               US          0         0         0   \n",
       "563                 US               US          0         1         0   \n",
       "564                 IN               US          0         1         0   \n",
       "\n",
       "    same_country  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "..           ...  \n",
       "560            1  \n",
       "561            1  \n",
       "562            1  \n",
       "563            1  \n",
       "564            0  \n",
       "\n",
       "[565 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_card_cat_features = [\n",
    "    'experience_level',  # EN, MI, SE, EX\n",
    "    'employment_type',   # FT, PT, CT, FL\n",
    "    'company_size',     # S, M, L\n",
    "]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), low_card_cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_processed = preprocessor.fit_transform(standardized_df)\n",
    "\n",
    "\n",
    "cat_encoder = preprocessor.named_transformers_['cat']\n",
    "feature_names = cat_encoder.get_feature_names_out(low_card_cat_features)\n",
    "\n",
    "all_feature_names = list(feature_names) + [\n",
    "    col for col in standardized_df.columns \n",
    "    if col not in low_card_cat_features\n",
    "]\n",
    "\n",
    "processed_df = pd.DataFrame(X_processed, columns=all_feature_names)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4e8fd-28c6-47cb-a531-9fa410447364",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> we need to figure out how we'll handle the job_title and employee_residence columns </span>\n",
    "\n",
    "**job_title:** this feaure seems like it will be important, as we suspect samples with the same/similar job title to have more similar salaries.\n",
    "\n",
    "**employee_residence:** this feature seems important, because salary often relates to cost of living (ie: the same position at the same company will pay different amounts depending on which US city it is located in).\n",
    "\n",
    "## <span style=\"color:limegreen\"> *for now, let's see what happens in we use label encoding* </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3085420-82c5-4dcc-96cf-9411367dfbe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# label_df = processed_df.copy()\n",
    "# label_df['job_title']= label_encoder.fit_transform(label_df['job_title'])\n",
    "# # label_df['employee_residence']=label_encoder.fit_transform(label_df['employee_residence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e799b9-6b1e-425c-b60d-2bcd10086e56",
   "metadata": {},
   "source": [
    "## <span style=\"color:limegreen\"> *okay lets try \"Bag of Words\" encoding job descriptions* </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c91c68-3096-4e77-8a1f-4b72a4541e37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'scientist', 'machine', 'learning', 'big', 'engineer', 'product', 'analyst', 'lead', 'business', 'science', 'consultant', 'bi', 'director', 'of', 'research', 'manager', 'engineering', 'infrastructure', 'ml', 'ai', 'computer', 'vision', 'principal', 'head', '3d', 'researcher', 'analytics', 'applied', 'marketing', 'cloud', 'financial', 'software', 'developer', 'specialist', 'architect', 'finance', 'staff', 'etl', 'nlp']\n",
      "---\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "processed_df[\"job_title_clean\"] = processed_df[\"job_title\"].str.lower().apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "unique_words = (processed_df['job_title_clean']\n",
    "                .str.split()  # Split into words (by whitespace)\n",
    "                .explode()    # Create one row per word\n",
    "                .unique()    # Get unique values\n",
    "                .tolist())\n",
    "print(unique_words)\n",
    "print(\"---\")\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f67068-d8fb-4962-b687-28fe14431c46",
   "metadata": {},
   "source": [
    "It seems like there are still a lot of possible words in the job titles. Let's normalize a couple (\"ml\" -> \"machine\" and \"learning\", \"computer\" and \"vision\" -> \"computer_vision\"\n",
    "\n",
    "Then we will apply BoW encoding for the top 20 title-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd2f311-b790-4e92-b34c-1722235383fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kevinyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize_terms(text):\n",
    "    \"\"\"\n",
    "    Replace specific terms/phrases before tokenization.\n",
    "    \"\"\"\n",
    "    # Convert \"ml\" to \"machine learning\" (case-insensitive)\n",
    "    text = re.sub(r'\\bml\\b', 'machine learning', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Combine \"computer vision\" as one term (handle with/without hyphen/space)\n",
    "    text = re.sub(r'\\bcomputer[-\\s]?vision\\b', 'computer_vision', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def get_word_frequency_df(df, column_name, min_count=0, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Create a DataFrame of word frequencies with term normalization.\n",
    "    \"\"\"\n",
    "    # Combine all text, normalize terms, and lowercase\n",
    "    all_text = ' '.join(df[column_name].dropna().astype(str))\n",
    "    all_text = normalize_terms(all_text).lower()\n",
    "    \n",
    "    # Tokenize words (hyphen/underscore-aware)\n",
    "    words = re.findall(r'\\b[a-z_]+(?:\\-[a-z_]+)*\\b', all_text)  # Matches hyphenated/underscore terms\n",
    "    \n",
    "    # Get stopwords and add custom ones\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if custom_stopwords:\n",
    "        stop_words.update([w.lower() for w in custom_stopwords])\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Filter words\n",
    "    filtered_counts = {\n",
    "        word: count for word, count in word_counts.items() \n",
    "        if word not in stop_words and count > min_count\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    word_df = (pd.DataFrame.from_dict(filtered_counts, orient='index', columns=['count'])\n",
    "               .reset_index()\n",
    "               .rename(columns={'index': 'word'})\n",
    "               .sort_values('count', ascending=False))\n",
    "    \n",
    "    return word_df\n",
    "\n",
    "word_freq_df = get_word_frequency_df(processed_df, 'job_title')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562a77d0-3581-4e8a-bb1f-beb2996ed3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_20_words = word_freq_df.head(20)['word'].tolist()\n",
    "\n",
    "\n",
    "def bow_encode(title, word_list):\n",
    "    title = title.lower()\n",
    "    features = {}\n",
    "    for word in word_list:\n",
    "        # Handle underscores (e.g., \"machine_learning\" → \"machine learning\")\n",
    "        search_term = word.replace('_', ' ')\n",
    "        features[f\"bow_{word}\"] = 1 if re.search(rf'\\b{search_term}\\b', title) else 0\n",
    "    return features\n",
    "\n",
    "# Apply to each job title and create a DataFrame\n",
    "bow_features = processed_df['job_title'].apply(lambda x: bow_encode(x, top_20_words)).apply(pd.Series)\n",
    "\n",
    "new_df = pd.concat([processed_df, bow_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c742bab1-e7aa-4b4a-a0df-8c4d0a4e1413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_level_EN</th>\n",
       "      <th>experience_level_EX</th>\n",
       "      <th>experience_level_MI</th>\n",
       "      <th>experience_level_SE</th>\n",
       "      <th>employment_type_CT</th>\n",
       "      <th>employment_type_FL</th>\n",
       "      <th>employment_type_FT</th>\n",
       "      <th>employment_type_PT</th>\n",
       "      <th>company_size_L</th>\n",
       "      <th>company_size_M</th>\n",
       "      <th>...</th>\n",
       "      <th>bow_lead</th>\n",
       "      <th>bow_principal</th>\n",
       "      <th>bow_architect</th>\n",
       "      <th>bow_computer_vision</th>\n",
       "      <th>bow_head</th>\n",
       "      <th>bow_director</th>\n",
       "      <th>bow_big</th>\n",
       "      <th>bow_applied</th>\n",
       "      <th>bow_ai</th>\n",
       "      <th>bow_engineering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>565 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    experience_level_EN experience_level_EX experience_level_MI  \\\n",
       "0                   0.0                 0.0                 1.0   \n",
       "1                   0.0                 0.0                 0.0   \n",
       "2                   0.0                 0.0                 0.0   \n",
       "3                   0.0                 0.0                 1.0   \n",
       "4                   0.0                 0.0                 0.0   \n",
       "..                  ...                 ...                 ...   \n",
       "560                 0.0                 0.0                 0.0   \n",
       "561                 0.0                 0.0                 0.0   \n",
       "562                 0.0                 0.0                 0.0   \n",
       "563                 0.0                 0.0                 0.0   \n",
       "564                 0.0                 0.0                 1.0   \n",
       "\n",
       "    experience_level_SE employment_type_CT employment_type_FL  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   1.0                0.0                0.0   \n",
       "2                   1.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   1.0                0.0                0.0   \n",
       "..                  ...                ...                ...   \n",
       "560                 1.0                0.0                0.0   \n",
       "561                 1.0                0.0                0.0   \n",
       "562                 1.0                0.0                0.0   \n",
       "563                 1.0                0.0                0.0   \n",
       "564                 0.0                0.0                0.0   \n",
       "\n",
       "    employment_type_FT employment_type_PT company_size_L company_size_M  ...  \\\n",
       "0                  1.0                0.0            1.0            0.0  ...   \n",
       "1                  1.0                0.0            0.0            0.0  ...   \n",
       "2                  1.0                0.0            0.0            1.0  ...   \n",
       "3                  1.0                0.0            0.0            0.0  ...   \n",
       "4                  1.0                0.0            1.0            0.0  ...   \n",
       "..                 ...                ...            ...            ...  ...   \n",
       "560                1.0                0.0            0.0            1.0  ...   \n",
       "561                1.0                0.0            0.0            1.0  ...   \n",
       "562                1.0                0.0            0.0            1.0  ...   \n",
       "563                1.0                0.0            0.0            1.0  ...   \n",
       "564                1.0                0.0            1.0            0.0  ...   \n",
       "\n",
       "    bow_lead bow_principal bow_architect bow_computer_vision bow_head  \\\n",
       "0          0             0             0                   0        0   \n",
       "1          0             0             0                   0        0   \n",
       "2          0             0             0                   0        0   \n",
       "3          0             0             0                   0        0   \n",
       "4          0             0             0                   0        0   \n",
       "..       ...           ...           ...                 ...      ...   \n",
       "560        0             0             0                   0        0   \n",
       "561        0             0             0                   0        0   \n",
       "562        0             0             0                   0        0   \n",
       "563        0             0             0                   0        0   \n",
       "564        0             0             0                   0        0   \n",
       "\n",
       "    bow_director bow_big bow_applied bow_ai bow_engineering  \n",
       "0              0       0           0      0               0  \n",
       "1              0       0           0      0               0  \n",
       "2              0       1           0      0               0  \n",
       "3              0       0           0      0               0  \n",
       "4              0       0           0      0               0  \n",
       "..           ...     ...         ...    ...             ...  \n",
       "560            0       0           0      0               0  \n",
       "561            0       0           0      0               0  \n",
       "562            0       0           0      0               0  \n",
       "563            0       0           0      0               0  \n",
       "564            0       0           0      1               0  \n",
       "\n",
       "[565 rows x 44 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed50c6-8542-40c8-af5c-73e1908724b2",
   "metadata": {},
   "source": [
    "## <span style=\"color:limegreen\"> *lets try target encoding for the employee_residence column* </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d244cf8-e3c2-4e69-9d48-ee07db2f4568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df['salary_in_usd'] = pd.to_numeric(new_df['salary_in_usd'])\n",
    "mean_employee_residence = new_df.groupby('employee_residence')['salary_in_usd'].mean().to_dict()\n",
    "new_df['employee_residence_mean_sal'] = new_df['employee_residence'].map(mean_employee_residence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b1cfab-11f9-4d92-9c7a-08657ce4888f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "new_df = new_df.drop(columns=['job_title', 'job_title_clean','salary_currency', 'employee_residence', 'company_location'])\n",
    "\n",
    "for col in new_df.select_dtypes(include=['object']).columns:\n",
    "    new_df[col] = pd.to_numeric(new_df[col], errors='raise').astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a10f6-0eab-4096-bab4-90551428357c",
   "metadata": {},
   "source": [
    "The last this to do before model building is to split the features from the target variable. As mentioned earlier, our target will be \"salary_in_usd\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba4f6a4a-2d0b-426d-a1cb-15f803b1ea79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = 'salary_in_usd'\n",
    "features = [col for col in new_df.columns if col not in target]\n",
    "\n",
    "# X = label_df[features]\n",
    "# y = label_df['salary_in_usd']\n",
    "\n",
    "X = new_df[features]\n",
    "y = new_df['salary_in_usd']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab48ff-60f0-4a20-9dc3-b340b347eee0",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b463c97-af3c-4738-91b8-a6b7d7d2c1a5",
   "metadata": {},
   "source": [
    "We are going to fit our model using gradient descent, so let's start by implmenting a multivariate linear GD algorithm with MSE as the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60c5c4-3110-4ee2-9125-a9784aba2799",
   "metadata": {},
   "source": [
    "### Vanilla GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a87df24-a2ab-4655-818e-84099c7076e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(grd_func, gamma, w0, N, tol=1e-05):\n",
    "    '''This function performs N-many iterations of gradient descent\n",
    "    \n",
    "    Args:\n",
    "        grd_func: the gradient of the function to minimize\n",
    "        gamma: the stepsize\n",
    "        w0: the starting x value/vector\n",
    "        N: the maximum number of iterations\n",
    "        (*) tol: the convergence tolerance\n",
    "\n",
    "    Output:\n",
    "        w: the final weight vector\n",
    "    '''\n",
    "\n",
    "    w = w0 # initialize w\n",
    "    \n",
    "    for n in range(N): # perform gradient descent N many times\n",
    "        grad = grd_func(w)\n",
    "\n",
    "        if np.linalg.norm(grad) < tol: # check if the function has converged (and return early if so)\n",
    "            print(f\"Converged!\\nIteration: {n}\")\n",
    "            return w\n",
    "\n",
    "        x_new = w - gamma*grad # gradient update rule\n",
    "        w = x_new #update w\n",
    "\n",
    "    print(f\"Did not converge...\\n Final Result: {w}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac017283-73d2-441b-8d44-545b7009538e",
   "metadata": {},
   "source": [
    "For now, we are using vanilla GD (ie a constant stepsize), although we might want to consider updating this if convergence takes too long... Let's see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cdc038c-35a7-42d1-b705-5618d138819d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Until we figure out a way to encode job_title, salary_currency, employee_residence, and company_location, we will drop those columns...\n",
    "# cols_to_drop = ['job_title', 'salary_currency', 'employee_residence', 'company_location']\n",
    "\n",
    "# X_subset = X.drop(columns=cols_to_drop)\n",
    "\n",
    "#did this above\n",
    "X_subset = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f36d674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged!\n",
      "Iteration: 132348\n",
      "w_star: [-0.01431125  1.39356277  0.25440476  0.51739099  0.93872302  0.17136581\n",
      "  0.56192311  0.47903533  0.50611428  0.37411981  0.27081318 -0.16929021\n",
      "  0.25517462 -0.51940691 -0.22851658 -0.36526153 -0.06705932 -0.0838587\n",
      " -0.15470847 -0.11831043 -0.5110981   0.05512339  0.05512339 -0.0719242\n",
      "  0.47456315  0.28435539 -0.10109246  0.87662949  1.17818016  0.3435191\n",
      " -0.1313245   0.274689    0.85471146 -0.08023009  0.75363024  0.21957198\n",
      " -0.2624812   0.88981468]\n",
      "b_star: -0.8489527275052151\n"
     ]
    }
   ],
   "source": [
    "def f_linear(X, w, b):\n",
    "    return w.T @ X + b\n",
    "X_aug = np.hstack([X_subset, np.ones((X_subset.shape[0], 1))]) # adding a column for the bias term\n",
    "\n",
    "def grad_mse_linear(w_aug):\n",
    "    n = X_aug.shape[0]\n",
    "    return (2/n) * X_aug.T @ (X_aug @ w_aug - y)\n",
    "\n",
    "w0_aug = np.ones(X_aug.shape[1])\n",
    "\n",
    "# Run gradient descent\n",
    "w_star_aug = gradient_descent(grad_mse_linear, gamma=0.01, w0=w0_aug, N=1000000)\n",
    "\n",
    "# Extract weights and bias\n",
    "w_star = w_star_aug[:-1]\n",
    "b_star = w_star_aug[-1]\n",
    "print(f'w_star: {w_star}')\n",
    "print(f'b_star: {b_star}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6499bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4079448381073953\n"
     ]
    }
   ],
   "source": [
    "lm_vanilla_preds = f_linear(X.T, w_star, b_star)\n",
    "lm_vanilla_mse = mean_squared_error(y, lm_vanilla_preds)\n",
    "print(lm_vanilla_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624768f2-2d5e-4611-b112-5f881fe1f2c0",
   "metadata": {},
   "source": [
    "Using vanilla GD is taking too long to converge. Thus, we should update the algorithm\n",
    "\n",
    "<span style=\"color:red\"> perhaps this is another thing to talk to Ben about. Is there standard practice for picking which gradient method to use?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a01b4-29af-4080-878d-d2f5c4a5a92c",
   "metadata": {},
   "source": [
    "### SGD (mini-batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "651885cb-c4b7-451f-a814-43cfc50febbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stochastic_gd(grd_func, gamma, w0, X, y, N, batch_size, tol=1e-5):\n",
    "    '''This function performs N-many iterations of stochastic gradient descent (SGD) for a model.\n",
    "    \n",
    "    Args:\n",
    "        grd_func: the gradient of the function to minimize\n",
    "        gamma: the stepsize\n",
    "        w0: the starting weight vector\n",
    "        X: the input data (Pandas DataFrame)\n",
    "        y: the output data (numpy array)\n",
    "        N: maximum number of iterations\n",
    "        batch_size: size of mini-batch for each SGD update\n",
    "        (*) tol: convergence tolerance\n",
    "\n",
    "    Output:\n",
    "        w: final weight vector after training\n",
    "    '''\n",
    "    \n",
    "    n = X.shape[0]  # number of samples\n",
    "    w = w0  # initialize weight vector\n",
    "\n",
    "    for i in range(N):\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(n)\n",
    "        X_shuffled = X[indices,:]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for j in range(0, n, batch_size):\n",
    "            X_batch = X_shuffled[j:j + batch_size,:]\n",
    "            y_batch = y_shuffled[j:j + batch_size]\n",
    "\n",
    "            # Compute gradients using the user-defined gradient function\n",
    "            grad = grd_func(w, X_batch, y_batch)\n",
    "\n",
    "            # Update weights using the gradient and learning rate\n",
    "            w_new = w - gamma * grad\n",
    "            w = w_new\n",
    "\n",
    "            # Convergence check (optional early stopping)\n",
    "            if np.linalg.norm(grad) < tol:\n",
    "                print(f\"Converged!\\nEpoch: {i}\")\n",
    "                return w\n",
    "\n",
    "    print(f\"Did not converge...\\nFinal Result: {w}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49cc9747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not converge...\n",
      "Final Result: [ 0.05254135  1.45894025  0.32120613  0.58226144  1.0068457   0.23774841\n",
      "  0.62307629  0.54727878  0.59452277  0.46201053  0.35841588 -0.16647227\n",
      "  0.16565163 -0.52250376 -0.14272665 -0.27667254 -0.07118287 -0.08639668\n",
      " -0.15686853 -0.12263553 -0.51430096  0.05433994  0.05433994 -0.07232723\n",
      "  0.47312157  0.2849159  -0.09804523  0.8784396   1.17775898  0.34219427\n",
      " -0.13047927  0.27372835  0.85220672 -0.08064097  0.75322178  0.22091028\n",
      " -0.26275401  0.88794092 -0.58505082 -0.58505082]\n",
      "w_star: [ 0.05254135  1.45894025  0.32120613  0.58226144  1.0068457   0.23774841\n",
      "  0.62307629  0.54727878  0.59452277  0.46201053  0.35841588 -0.16647227\n",
      "  0.16565163 -0.52250376 -0.14272665 -0.27667254 -0.07118287 -0.08639668\n",
      " -0.15686853 -0.12263553 -0.51430096  0.05433994  0.05433994 -0.07232723\n",
      "  0.47312157  0.2849159  -0.09804523  0.8784396   1.17775898  0.34219427\n",
      " -0.13047927  0.27372835  0.85220672 -0.08064097  0.75322178  0.22091028\n",
      " -0.26275401  0.88794092 -0.58505082]\n",
      "b_star: -0.5850508205620837\n"
     ]
    }
   ],
   "source": [
    "def sgd_grad_linear_mse(w, X_batch, y_batch):\n",
    "    X_aug = np.hstack([X_batch, np.ones((X_batch.shape[0], 1))])  # Adding bias as the last column\n",
    "    n = len(X_aug)\n",
    "        \n",
    "    y_pred = X_aug @ w\n",
    "    error = y_batch - y_pred\n",
    "    \n",
    "    return -(2/n) * X_aug.T @ error  # gradient with respect to w (including bias)\n",
    "\n",
    "w0_aug = np.ones(X_aug.shape[1]+1)\n",
    "\n",
    "w_star_aug = stochastic_gd(sgd_grad_linear_mse, gamma=0.01, w0=w0_aug, X=X_aug, y=y, N=1000000, batch_size=25)\n",
    "\n",
    "# Extract weights and bias\n",
    "w_star = w_star_aug[:-1]\n",
    "b_star = w_star_aug[-1]\n",
    "print(f'w_star: {w_star}')\n",
    "print(f'b_star: {b_star}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1b438e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7180236380805872\n"
     ]
    }
   ],
   "source": [
    "w_star = np.array([0.05254135, 1.45894025, 0.32120613, 0.58226144, 1.0068457, 0.23774841, 0.62307629, 0.54727878, 0.59452277, 0.46201053, 0.35841588, -0.16647227,\n",
    "          0.16565163, -0.52250376, -0.14272665, -0.27667254, -0.07118287, -0.08639668, -0.15686853, -0.12263553, -0.51430096,  0.05433994,  0.05433994,\n",
    "          -0.07232723, 0.47312157, 0.2849159, -0.09804523, 0.8784396, 1.17775898, 0.34219427, -0.13047927, 0.27372835, 0.85220672, -0.08064097,\n",
    "          0.75322178,  0.22091028, -0.26275401,  0.88794092])\n",
    "b_star = -0.5850508205620837\n",
    "\n",
    "lm_sgd_preds = f_linear(X.T, w_star, b_star)\n",
    "lm_sgd_mse = mean_squared_error(y, lm_sgd_preds)\n",
    "print(lm_sgd_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f1efd",
   "metadata": {},
   "source": [
    "### AGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7e7b7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accelerated_gd(grd_func, gamma, w0, N, momentum=0.9, tol=1e-05):\n",
    "    '''This function performs N-many iterations of momentum-based gradient descent\n",
    "    \n",
    "    Args:\n",
    "        grd_func: the gradient of the function to minimize\n",
    "        gamma: the stepsize\n",
    "        w0: the starting x value/vector\n",
    "        N: the maximum number of iterations\n",
    "        (*) momentum: the momentum parameter (usually between 0 and 1)\n",
    "        (*) tol: the convergence tolerance\n",
    "\n",
    "    Output:\n",
    "        w: the final weight vector\n",
    "    '''\n",
    "\n",
    "    w = w0\n",
    "    v = np.zeros_like(w0)  # initialize velocity (same shape as w0)\n",
    "    \n",
    "    for n in range(N):\n",
    "        grad = grd_func(w)\n",
    "\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"Converged!\\nIteration: {n}\")\n",
    "            return w\n",
    "\n",
    "        v = momentum * v - gamma * grad\n",
    "        w = w + v \n",
    "\n",
    "    print(f\"Did not converge...\\n Final Result: {w}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce1c879b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged!\n",
      "Iteration: 13176\n",
      "w_star: [-0.01431122  1.39356285  0.25440479  0.51739103  0.93872312  0.17136587\n",
      "  0.56192314  0.47903531  0.50611434  0.37411987  0.27081324 -0.16929021\n",
      "  0.25517456 -0.51940712 -0.22851653 -0.36526147 -0.06705932 -0.08385855\n",
      " -0.15470895 -0.11831094 -0.51109861  0.05512345  0.05512345 -0.07192446\n",
      "  0.47456307  0.28435558 -0.10109257  0.87662969  1.17818014  0.34351852\n",
      " -0.13132437  0.27468874  0.85471139 -0.0802301   0.75363027  0.21957225\n",
      " -0.26248149  0.88981469]\n",
      "b_star: -0.8489525557829265\n"
     ]
    }
   ],
   "source": [
    "def f_linear(X, w, b):\n",
    "    return w.T @ X + b\n",
    "\n",
    "X_aug = np.hstack([X_subset, np.ones((X_subset.shape[0], 1))]) # adding a column for the bias term\n",
    "\n",
    "def grad_mse_linear(w_aug):\n",
    "    n = X_aug.shape[0]\n",
    "    return (2/n) * X_aug.T @ (X_aug @ w_aug - y)\n",
    "\n",
    "w0_aug = np.ones(X_aug.shape[1])\n",
    "\n",
    "# Run gradient descent\n",
    "w_star_aug = accelerated_gd(grad_mse_linear, gamma=0.01, w0=w0_aug, N=100000)\n",
    "\n",
    "# Extract weights and bias\n",
    "w_star = w_star_aug[:-1]\n",
    "b_star = w_star_aug[-1]\n",
    "print(f'w_star: {w_star}')\n",
    "print(f'b_star: {b_star}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e14cbd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40794483810083954\n"
     ]
    }
   ],
   "source": [
    "lm_agm_preds = f_linear(X.T, w_star, b_star)\n",
    "lm_agm_mse = mean_squared_error(y, lm_agm_preds)\n",
    "print(lm_agm_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b236fa0-934e-43ae-86e4-efb4e1a7317d",
   "metadata": {},
   "source": [
    "# Ridge Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c030c1-5b40-497f-bd89-87d5fa2ef49c",
   "metadata": {},
   "source": [
    "To avoid overfitting, we'll implement ridge regression, which essentially modifies the gradient function to include the L2 regularization term. This is also beneficial when we have a lot of features.\n",
    "We will also be performing cross validation to find the optimal lambda value for each optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c2aa67f-d179-42a7-b262-4304a05fcf39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_ridge_linear(w_aug, X_aug, y, lambda_): #new gradient function\n",
    "    '''Gradient for Ridge Regression'''\n",
    "    n = X_aug.shape[0]\n",
    "    grad = (2/n) * X_aug.T @ (X_aug @ w_aug - y) + lambda_ * w_aug\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2e048-fe5e-49c9-9d73-c72b07eb47fc",
   "metadata": {},
   "source": [
    "### Vanilla GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bcb3c513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD did not converge.\n",
      "GD Converged at iteration 6750\n",
      "GD Converged at iteration 6428\n",
      "GD Converged at iteration 6433\n",
      "GD Converged at iteration 6710\n",
      "GD Converged at iteration 6507\n",
      "GD Converged at iteration 868\n",
      "GD Converged at iteration 877\n",
      "GD Converged at iteration 876\n",
      "GD Converged at iteration 864\n",
      "GD Converged at iteration 858\n",
      "GD Converged at iteration 100\n",
      "GD Converged at iteration 100\n",
      "GD Converged at iteration 100\n",
      "GD Converged at iteration 100\n",
      "GD Converged at iteration 100\n",
      "GD Converged at iteration 5\n",
      "GD Converged at iteration 5\n",
      "GD Converged at iteration 5\n",
      "GD Converged at iteration 5\n",
      "GD Converged at iteration 5\n",
      "Optimal lambda (GD): 0.01\n",
      "GD Converged at iteration 35267\n",
      "w_star: [-0.49215169  0.80305956 -0.23948001  0.02955682  0.31169948 -0.21225251\n",
      "  0.03844421 -0.0369065   0.16261157  0.01989767 -0.08152455 -0.14962576\n",
      " -0.05347424  0.11686953  0.09865834 -0.05114789 -0.04977702 -0.10813222\n",
      " -0.05388842  0.01318179 -0.38831221  0.04842487  0.04842487  0.01464014\n",
      " -0.12441529  0.19377591 -0.04903701  0.20407963  0.95490125  0.35706609\n",
      " -0.12673987 -0.18321681  0.22042199 -0.05165138  0.55393867  0.11173119\n",
      " -0.12485272  0.87709999]\n",
      "b_star: 0.10098468557697184\n"
     ]
    }
   ],
   "source": [
    "def gd_ridge(w0, X_aug, y, N, gamma=0.01, lambda_=0.01, tol=1e-5):\n",
    "    '''Vanilla Gradient Descent for Ridge Regression'''\n",
    "    w = w0.copy()\n",
    "    \n",
    "    for i in range(N):\n",
    "        grad = grad_ridge_linear(w, X_aug, y, lambda_)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"GD Converged at iteration {i}\")\n",
    "            return w\n",
    "        \n",
    "        w = w - gamma * grad\n",
    "\n",
    "    print(\"GD did not converge.\")\n",
    "    return w\n",
    "\n",
    "def cross_val_gd_ridge(X, y, lambda_values, N=10000, gamma=0.01, k_folds=5):\n",
    "    '''Cross-validation to select optimal lambda using vanilla GD'''\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    best_lambda = None\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    for lambda_ in lambda_values:\n",
    "        mse_list = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train_raw, X_val_raw = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Augment for bias\n",
    "            X_train = np.hstack([X_train_raw, np.ones((X_train_raw.shape[0], 1))])\n",
    "            X_val = np.hstack([X_val_raw, np.ones((X_val_raw.shape[0], 1))])\n",
    "            w0_aug = np.zeros(X_train.shape[1])\n",
    "\n",
    "            # Train using vanilla GD\n",
    "            w_star_aug = gd_ridge(\n",
    "                w0=w0_aug, X_aug=X_train, y=y_train,\n",
    "                N=N, gamma=gamma, lambda_=lambda_\n",
    "            )\n",
    "\n",
    "            # Predict and compute MSE\n",
    "            y_pred = X_val @ w_star_aug\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mse_list.append(mse)\n",
    "\n",
    "        avg_mse = np.mean(mse_list)\n",
    "        if avg_mse < best_mse:\n",
    "            best_mse = avg_mse\n",
    "            best_lambda = lambda_\n",
    "\n",
    "    return best_lambda\n",
    "\n",
    "lambda_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "best_lambda = cross_val_gd_ridge(X, y, lambda_values, N=10000, gamma=0.01)\n",
    "\n",
    "print(f\"Optimal lambda (GD): {best_lambda}\")\n",
    "\n",
    "# Train on full data\n",
    "X_aug = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "w0_aug = np.zeros(X_aug.shape[1])\n",
    "\n",
    "w_star_aug = gd_ridge(w0=w0_aug, X_aug=X_aug, y=y, N=100000, gamma=0.01, lambda_=best_lambda)\n",
    "\n",
    "w_star = w_star_aug[:-1]\n",
    "b_star = w_star_aug[-1]\n",
    "\n",
    "print(f\"w_star: {w_star}\")\n",
    "print(f\"b_star: {b_star}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "651dd664-f2bd-418d-aeb4-2d64c884862f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48708881040531915\n"
     ]
    }
   ],
   "source": [
    "def f_linear(X, w, b):\n",
    "    return w.T @ X + b\n",
    "\n",
    "lmr_vanilla_preds = f_linear(X.T, w_star, b_star)\n",
    "lmr_vanilla_mse = mean_squared_error(y, lmr_vanilla_preds)\n",
    "print(lmr_sgd_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3a3f28d-7aca-4475-8306-bed2a540f163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "SGD did not converge. Final weights returned.\n",
      "Optimal lambda (SGD): 0.1\n",
      "SGD did not converge. Final weights returned.\n",
      "w_star: [-0.351642    0.38909162 -0.21573899  0.17031193  0.07960694 -0.05778351\n",
      "  0.01458246 -0.04438333  0.20789409 -0.06006202 -0.15580951  0.00262798\n",
      "  0.03269773  0.17745967  0.18089401 -0.15617372 -0.03765383 -0.047908\n",
      " -0.07012743  0.0476749  -0.2821823  -0.04383788 -0.04383788  0.11794078\n",
      " -0.03152565  0.02497073  0.04038008  0.11465631  0.32032322  0.16608675\n",
      " -0.02788683 -0.03515318  0.12948219  0.0027194   0.12612031  0.05916354\n",
      " -0.03560353  0.61896996]\n",
      "b_star: -0.007977437703776569\n",
      "0.4928791295252101\n"
     ]
    }
   ],
   "source": [
    "def sgd_ridge(w0, X_aug, y, N, gamma=0.01, lambda_=0.01, batch_size=1, tol=1e-5):\n",
    "    '''Stochastic Gradient Descent for Ridge Regression'''\n",
    "    w = w0.copy()\n",
    "    n = X_aug.shape[0]\n",
    "\n",
    "    for i in range(N):\n",
    "        # Sample a minibatch\n",
    "        idx = np.random.choice(n, batch_size, replace=False)\n",
    "        X_batch = X_aug[idx]\n",
    "        y_batch = y[idx]\n",
    "\n",
    "        # Compute gradient on minibatch\n",
    "        grad = (2/batch_size) * X_batch.T @ (X_batch @ w - y_batch) + lambda_ * w\n",
    "\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"SGD Converged at iteration {i}\")\n",
    "            return w\n",
    "\n",
    "        # Update\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    print(f\"SGD did not converge. Final weights returned.\")\n",
    "    return w\n",
    "\n",
    "def cross_val_sgd_ridge(X, y, lambda_values, N=10000, gamma=0.01, batch_size=1, k_folds=5):\n",
    "    '''Cross-validation to select optimal lambda using SGD'''\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    best_lambda = None\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    for lambda_ in lambda_values:\n",
    "        mse_list = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train_raw, X_val_raw = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Augment matrices\n",
    "            X_train = np.hstack([X_train_raw, np.ones((X_train_raw.shape[0], 1))])\n",
    "            X_val = np.hstack([X_val_raw, np.ones((X_val_raw.shape[0], 1))])\n",
    "\n",
    "            w0_aug = np.zeros(X_train.shape[1])\n",
    "\n",
    "            # Train using SGD\n",
    "            w_star_aug = sgd_ridge(\n",
    "                w0=w0_aug, X_aug=X_train, y=y_train,\n",
    "                N=N, gamma=gamma, lambda_=lambda_,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "\n",
    "            # Predict and compute MSE\n",
    "            y_pred = X_val @ w_star_aug\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mse_list.append(mse)\n",
    "\n",
    "        avg_mse = np.mean(mse_list)\n",
    "        if avg_mse < best_mse:\n",
    "            best_mse = avg_mse\n",
    "            best_lambda = lambda_\n",
    "\n",
    "    return best_lambda\n",
    "lambda_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "best_lambda = cross_val_sgd_ridge(X, y, lambda_values, N=10000, gamma=0.01, batch_size=1)\n",
    "\n",
    "print(f\"Optimal lambda (SGD): {best_lambda}\")\n",
    "\n",
    "# Retrain on full dataset\n",
    "X_aug = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "w0_aug = np.zeros(X_aug.shape[1])\n",
    "\n",
    "w_star_aug = sgd_ridge(\n",
    "    w0=w0_aug, X_aug=X_aug, y=y,\n",
    "    N=100000, gamma=0.01, lambda_=best_lambda,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "w_star = w_star_aug[:-1]\n",
    "b_star = w_star_aug[-1]\n",
    "\n",
    "print(f\"w_star: {w_star}\")\n",
    "print(f\"b_star: {b_star}\")\n",
    "\n",
    "lm_agm_preds = f_linear(X.T, w_star, b_star)\n",
    "lm_agm_mse = mean_squared_error(y, lm_agm_preds)\n",
    "print(lm_agm_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97260d00-4c11-4fd8-bce2-31578a1e80f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged!\n",
      "Iteration: 9981\n",
      "Converged!\n",
      "Iteration: 6907\n",
      "Converged!\n",
      "Iteration: 8967\n",
      "Converged!\n",
      "Iteration: 9268\n",
      "Converged!\n",
      "Iteration: 7280\n",
      "Converged!\n",
      "Iteration: 4017\n",
      "Converged!\n",
      "Iteration: 3231\n",
      "Converged!\n",
      "Iteration: 3829\n",
      "Converged!\n",
      "Iteration: 3942\n",
      "Converged!\n",
      "Iteration: 3113\n",
      "Converged!\n",
      "Iteration: 597\n",
      "Converged!\n",
      "Iteration: 565\n",
      "Converged!\n",
      "Iteration: 566\n",
      "Converged!\n",
      "Iteration: 593\n",
      "Converged!\n",
      "Iteration: 571\n",
      "Converged!\n",
      "Iteration: 206\n",
      "Converged!\n",
      "Iteration: 204\n",
      "Converged!\n",
      "Iteration: 205\n",
      "Converged!\n",
      "Iteration: 204\n",
      "Converged!\n",
      "Iteration: 205\n",
      "Converged!\n",
      "Iteration: 213\n",
      "Converged!\n",
      "Iteration: 206\n",
      "Converged!\n",
      "Iteration: 206\n",
      "Converged!\n",
      "Iteration: 205\n",
      "Converged!\n",
      "Iteration: 213\n",
      "Converged!\n",
      "Iteration: 204\n",
      "Converged!\n",
      "Iteration: 204\n",
      "Converged!\n",
      "Iteration: 204\n",
      "Converged!\n",
      "Iteration: 204\n",
      "Converged!\n",
      "Iteration: 204\n",
      "Optimal lambda: 0.01\n",
      "Converged!\n",
      "Iteration: 3481\n",
      "w_star: [-0.49215182  0.80305943 -0.23948008  0.02955672  0.3117005  -0.21225432\n",
      "  0.03844433 -0.03690627  0.16261142  0.0198975  -0.08152467 -0.1496258\n",
      " -0.05347411  0.11686999  0.09865818 -0.05114804 -0.04977706 -0.10813162\n",
      " -0.05388819  0.01318193 -0.38831205  0.04842515  0.04842515  0.01463959\n",
      " -0.12441486  0.19377658 -0.04903726  0.20407899  0.95490117  0.35706635\n",
      " -0.12673902 -0.18321693  0.22042278 -0.05165162  0.55393873  0.11173196\n",
      " -0.12485437  0.87709999]\n",
      "b_star: 0.10098424605073351\n"
     ]
    }
   ],
   "source": [
    "def accelerated_gd_ridge(grd_func, gamma, w0, X_aug, y, N, lambda_, momentum=0.9, tol=1e-5):\n",
    "    '''AGM for Ridge Regression'''\n",
    "    w = w0\n",
    "    v = np.zeros_like(w0)\n",
    "\n",
    "    for n in range(N):\n",
    "        grad = grd_func(w, X_aug, y, lambda_)\n",
    "\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"Converged!\\nIteration: {n}\")\n",
    "            return w\n",
    "\n",
    "        v = momentum * v - gamma * grad\n",
    "        w = w + v\n",
    "\n",
    "    print(f\"Did not converge...\\n Final Result: {w}\")\n",
    "    return w\n",
    "\n",
    "def cross_val_agm_ridge(X, y, lambda_values, N=10000, gamma=0.01, momentum=0.9, k_folds=5):\n",
    "    '''Cross-validation for Ridge Regression to find best lambda'''\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_lambda = None\n",
    "    best_mse = float('inf')\n",
    "    \n",
    "    for lambda_ in lambda_values:\n",
    "        mse_list = []\n",
    "        \n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train_raw, X_val_raw = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            # Augment design matrices\n",
    "            X_train = np.hstack([X_train_raw, np.ones((X_train_raw.shape[0], 1))])\n",
    "            X_val = np.hstack([X_val_raw, np.ones((X_val_raw.shape[0], 1))])\n",
    "\n",
    "            # Initialize weight vector (match augmented X_train)\n",
    "            w0_aug = np.zeros(X_train.shape[1])\n",
    "\n",
    "            # Run AGM with Ridge\n",
    "            w_star_aug = accelerated_gd_ridge(\n",
    "                grad_ridge_linear, gamma=gamma, w0=w0_aug,\n",
    "                X_aug=X_train, y=y_train, N=N, lambda_=lambda_, momentum=momentum\n",
    "            )\n",
    "\n",
    "            # Predict\n",
    "            y_pred = X_val @ w_star_aug\n",
    "\n",
    "            # Compute MSE\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mse_list.append(mse)\n",
    "        \n",
    "        avg_mse = np.mean(mse_list)\n",
    "\n",
    "        if avg_mse < best_mse:\n",
    "            best_mse = avg_mse\n",
    "            best_lambda = lambda_\n",
    "\n",
    "    return best_lambda\n",
    "\n",
    "\n",
    "X_aug = np.hstack([X, np.ones((X.shape[0], 1))])  # X with a bias column of 1s\n",
    "\n",
    "# Initialize weight vector for the augmented X matrix\n",
    "w0_aug = np.zeros(X_aug.shape[1]) \n",
    "\n",
    "lambda_values = [0.001, 0.01, 0.1, 1, 10, 100]  # Range of lambda values\n",
    "best_lambda = cross_val_agm_ridge(X_aug, y, lambda_values)\n",
    "\n",
    "print(f\"Optimal lambda: {best_lambda}\")\n",
    "\n",
    "\n",
    "# After finding the optimal lambda\n",
    "w_star_aug = accelerated_gd_ridge(grad_ridge_linear, gamma=0.01, w0=w0_aug, X_aug=X_aug, y=y, N=100000, lambda_=best_lambda)\n",
    "\n",
    "# Extract weights and bias\n",
    "w_star = w_star_aug[:-1]\n",
    "b_star = w_star_aug[-1]\n",
    "print(f'w_star: {w_star}')\n",
    "print(f'b_star: {b_star}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e8b3f05-220c-4045-8c33-5b5d8f35f95b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41111503097725993\n"
     ]
    }
   ],
   "source": [
    "lmr_agm_preds = f_linear(X.T, w_star, b_star)\n",
    "lmr_agm_mse = mean_squared_error(y, lmr_agm_preds)\n",
    "print(lmr_agm_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d8898-d20a-4f23-a504-3fb496adf441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
